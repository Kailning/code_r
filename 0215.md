# 地址空间
## 实现 SV39 多级页表机制
### 虚拟地址和物理地址
#### 内存控制相关的CSR寄存器
默认情况下 MMU 未被使能，此时无论 CPU 处于哪个特权级，访存的地址都将直接被视作物理地址。 可以通过修改 S 特权级的 satp CSR 来启用分页模式，此后 S 和 U 特权级的访存地址会被视为虚拟地址，经过 MMU 的地址转换获得对应物理地址，再通过它来访问物理内存。
#### 地址格式与组成
![image-9](https://github.com/Kalining/code_repository/assets/148835940/d68f35aa-514f-4449-a376-354daaf21622)

采用分页管理，单个页面的大小设置为 4KiB，每个虚拟页面和物理页帧都按 4 KB 对齐。 
 需要用 12 位字节地址来表示，因此虚拟地址和物理地址都被分成两部分：它们的低 12 位被称为 页内偏移 (Page Offset) 。虚拟地址的高 27 位，即 [38:12] 为它的虚拟页号 VPN； 物理地址的高 44 位，即 [55:12] 为它的物理页号 PPN。页号可以用来定位一个虚拟/物理地址属于哪一个虚拟页面/物理页帧。    
地址转换是以页为单位进行的，转换前后地址页内偏移部分不变。MMU 只是从虚拟地址中取出 27 位虚拟页号， 在页表中查到其对应的物理页号，如果找到，就将得到的 44 位的物理页号与 12 位页内偏移拼接到一起，形成 56 位物理地址。
#### 地址相关的数据结构抽象与类型定义
+ 将地址和页号的概念抽象为 Rust 中的类型
  ```
  #[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq)]
  pub struct PhysAddr(pub usize);//物理地址

  #[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq)]
  pub struct VirtAddr(pub usize);//虚拟地址

  #[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq)]
  pub struct PhysPageNum(pub usize);//物理页号

  #[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq)]
  pub struct VirtPageNum(pub usize);//虚拟页号
  ```
  usize 的一种简单包装。 将它们各自抽象出来而不是直接使用 usize，是为了在 Rust 编译器的帮助下进行多种方便且安全的 类型转换 (Type Convertion) 。
  这些类型本身可以和 usize 之间互相转换，地址和页号之间也可以相互转换。
+ 物理地址和物理页号之间的转换：
  ```
  impl PhysAddr {
     pub fn page_offset(&self) -> usize { self.0 & (PAGE_SIZE - 1) }
  
  impl From<PhysAddr> for PhysPageNum {
      fn from(v: PhysAddr) -> Self {
          assert_eq!(v.page_offset(), 0);
          v.floor()
      }
  }

  impl From<PhysPageNum> for PhysAddr {
      fn from(v: PhysPageNum) -> Self { Self(v.0 << PAGE_SIZE_BITS) }
  }
  ```
  PAGE_SIZE 为 4096， PAGE_SIZE_BITS 为 12，它们均定义在 config 子模块中,分别表示每个页面的大小和页内偏移的位宽。从物理页号到物理地址的转换只需左移 
  位即可，但是物理地址需要 保证它与页面大小对齐才能通过右移转换为物理页号。
+ 对于不对齐的情况，物理地址不能通过 From/Into 转换为物理页号，而是需要通过它自己的 floor 或 ceil 方法来 进行下取整或上取整的转换。
  ```
  impl PhysAddr {
    pub fn floor(&self) -> PhysPageNum { PhysPageNum(self.0 / PAGE_SIZE) }
    pub fn ceil(&self) -> PhysPageNum { PhysPageNum((self.0 + PAGE_SIZE - 1) / PAGE_SIZE) }
  }
 
### 页表项的数据结构抽象与类型定义
![image-10](https://github.com/Kalining/code_repository/assets/148835940/c13478db-88d6-44f9-b9e3-290d4fda1c40)

上图为 SV39 分页模式下的页表项，其中 [53:10] 这 44 位是物理页号，最低的 8 位 [7:0] 则是标志位，它们的含义如下：
+ 仅当 V(Valid) 位为 1 时，页表项才是合法的；
+ R/W/X 分别控制索引到这个页表项的对应虚拟页面是否允许读/写/取指；
+ U 控制索引到这个页表项的对应虚拟页面是否在 CPU 处于 U 特权级的情况下是否被允许访问；
+ A(Accessed) 记录自从页表项上的这一位被清零之后，页表项的对应虚拟页面是否被访问过；
+ D(Dirty) 则记录自从页表项上的这一位被清零之后，页表项的对应虚拟页表是否被修改过。
实现页表项中的标志位 PTEFlags ：
```
// os/src/main.rs

#[macro_use]
extern crate bitflags;

// os/src/mm/page_table.rs

use bitflags::*;

bitflags! {
    pub struct PTEFlags: u8 {
        const V = 1 << 0;
        const R = 1 << 1;
        const W = 1 << 2;
        const X = 1 << 3;
        const U = 1 << 4;
        const G = 1 << 5;
        const A = 1 << 6;
        const D = 1 << 7;
    }
}
```
bitflags 是一个 Rust 中常用来比特标志位的 crate 。它提供了 一个 bitflags! 宏，如上面的代码段所展示的那样，可以将一个 u8 封装成一个标志位的集合类型，支持一些常见的集合 运算。
实现页表项 PageTableEntry ：
```
1// os/src/mm/page_table.rs
 2
 3#[derive(Copy, Clone)]
 4#[repr(C)]
 5pub struct PageTableEntry {
 6    pub bits: usize,
 7}
 8
 9impl PageTableEntry {
10    pub fn new(ppn: PhysPageNum, flags: PTEFlags) -> Self {
11        PageTableEntry {
12            bits: ppn.0 << 10 | flags.bits as usize,
13        }
14    }
15    pub fn empty() -> Self {
16        PageTableEntry {
17            bits: 0,
18        }
19    }
20    pub fn ppn(&self) -> PhysPageNum {
21        (self.bits >> 10 & ((1usize << 44) - 1)).into()
22    }
23    pub fn flags(&self) -> PTEFlags {
24        PTEFlags::from_bits(self.bits as u8).unwrap()
25    }
26}
```
为 PageTableEntry 实现一些辅助函数(Helper Function)，可以快速判断一个页表项的 V/R/W/X 标志位是否为 1，以 V 标志位的判断为例：
```
// os/src/mm/page_table.rs
//判断两个集合的交集是否为空
impl PageTableEntry {
    pub fn is_valid(&self) -> bool {
        (self.flags() & PTEFlags::V) != PTEFlags::empty()
    }
}
```
### 物理页帧管理
#### 可用物理页的分配与回收
在 os/src/linker.ld 中，用符号 ekernel 指明 内核数据的终止物理地址，在它之后的物理内存都是可用的。而在 config 子模块中：
```
// os/src/config.rs
pub const MEMORY_END: usize = 0x80800000;
```
硬编码整块物理内存的终止物理地址为 0x80800000 。 而物理内存的起始物理地址为 0x80000000 ， 意味着我们将可用内存大小设置为 8MiB(也可以设置的更大一点)。
用一个左闭右开的物理页号区间来表示可用的物理内存，则：
+ 区间的左端点应该是 ekernel 的物理地址以上取整方式转化成的物理页号；
+ 区间的右端点应该是 MEMORY_END 以下取整方式转化成的物理页号。
这个区间将被传给我们后面实现的物理页帧管理器用于初始化。
声明一个 FrameAllocator Trait 来描述一个物理页帧管理器需要提供哪些功能：
```
// os/src/mm/frame_allocator.rs

trait FrameAllocator {
    fn new() -> Self;
    fn alloc(&mut self) -> Option<PhysPageNum>;
    fn dealloc(&mut self, ppn: PhysPageNum);
}
```
我们实现一种最简单的栈式物理页帧管理策略 StackFrameAllocator ：
```
// os/src/mm/frame_allocator.rs

pub struct StackFrameAllocator {
    current: usize,
    end: usize,
    recycled: Vec<usize>,
}
```
初始化非常简单。在通过 FrameAllocator 的 new 方法创建实例的时候，只需将区间两端均设为 0 ， 然后创建一个新的向量；而在它真正被使用起来之前，需要调用 init 方法将自身的 [current,end) 初始化为可用物理页号区间：
```
// os/src/mm/frame_allocator.rs

impl FrameAllocator for StackFrameAllocator {
    fn new() -> Self {
        Self {
            current: 0,
            end: 0,
            recycled: Vec::new(),
        }
    }
}

impl StackFrameAllocator {
    pub fn init(&mut self, l: PhysPageNum, r: PhysPageNum) {
        self.current = l.0;
        self.end = r.0;
    }
}
```
核心的物理页帧分配和回收实现：
```
// os/src/mm/frame_allocator.rs

impl FrameAllocator for StackFrameAllocator {
    fn alloc(&mut self) -> Option<PhysPageNum> {
        if let Some(ppn) = self.recycled.pop() {
            Some(ppn.into())
        } else {
            if self.current == self.end {
                None
            } else {
                self.current += 1;
                Some((self.current - 1).into())
            }
        }
    }
    fn dealloc(&mut self, ppn: PhysPageNum) {
        let ppn = ppn.0;
        // validity check
        if ppn >= self.current || self.recycled
            .iter()
            .find(|&v| {*v == ppn})
            .is_some() {
            panic!("Frame ppn={:#x} has not been allocated!", ppn);
        }
        // recycle
        self.recycled.push(ppn);
    }
}
```
+ 在分配 alloc 的时候，首先会检查栈 recycled 内有没有之前回收的物理页号，如果有的话直接弹出栈顶并返回； 否则的话只能从之前从未分配过的物理页号区间[current,end) 上进行分配，分配它的 左端点 current ，同时将管理器内部维护的 current 加一代表 current 此前已经被分配过了。在即将返回 的时候，使用 into 方法将 usize 转换成了物理页号 PhysPageNum 。

极端情况下可能出现内存耗尽分配失败的情况：即 recycled 为空且 current==end。 为了涵盖这种情况， alloc 的返回值被 Option 包裹，我们返回 None 即可。

+ 在回收 dealloc 的时候，我们需要检查回收页面的合法性，然后将其压入 recycled 栈中。回收页面合法有两个 条件：

  + 该页面之前一定被分配出去过，因此它的物理页号一定 <current；

  + 该页面没有正处在回收状态，即它的物理页号不能在栈 recycled 中找到。

通过 recycled.iter() 获取栈上内容的迭代器，然后通过迭代器的 find 方法试图 寻找一个与输入物理页号相同的元素。其返回值是一个 Option ，如果找到了就会是一个 Option::Some ， 这种情况说明内核其他部分实现有误，直接报错退出。

之后创建 StackFrameAllocator 的全局实例 FRAME_ALLOCATOR，并在正式分配物理页帧之前将 FRAME_ALLOCATOR 初始化(见 os/src/mm/frame_allocator.rs)。

#### 分配/回收物理页帧的接口
公开给其他子模块调用的分配/回收物理页帧的接口：
```
// os/src/mm/frame_allocator.rs

pub fn frame_alloc() -> Option<FrameTracker> {
    FRAME_ALLOCATOR
        .exclusive_access()
        .alloc()
        .map(FrameTracker::new)
}

fn frame_dealloc(ppn: PhysPageNum) {
    FRAME_ALLOCATOR.exclusive_access().dealloc(ppn);
}
```
可以发现， frame_alloc 的返回值类型并不是 FrameAllocator 要求的物理页号 PhysPageNum ，而是将其 进一步包装为一个 FrameTracker ，其定义如下。 FrameTracker 被创建时，需要从 FRAME_ALLOCATOR 中分配一个物理页帧：
```
// os/src/mm/frame_allocator.rs

pub struct FrameTracker {
    pub ppn: PhysPageNum,
}

impl FrameTracker {
    pub fn new(ppn: PhysPageNum) -> Self {
        // page cleaning
        let bytes_array = ppn.get_bytes_array();
        for i in bytes_array {
            *i = 0;
        }
        Self { ppn }
    }
}
```
将分配来的物理页帧的物理页号作为参数传给 FrameTracker 的 new 方法来创建一个 FrameTracker 实例。由于这个物理页帧之前可能被分配过并用做其他用途，在这里直接将这个物理页帧上的所有字节清零。

当一个 FrameTracker 生命周期结束被编译器回收的时候，需要将它控制的物理页帧回收掉 FRAME_ALLOCATOR 中：
```
// os/src/mm/frame_allocator.rs

impl Drop for FrameTracker {
    fn drop(&mut self) {
        frame_dealloc(self.ppn);
    }
}
```
这里只需为 FrameTracker 实现 Drop Trait 即可。当一个 FrameTracker 实例被回收的时候，它的 drop 方法会自动被编译器调用，通过之前实现的 frame_dealloc 就将它控制的物理页帧回收以供后续使用了。
/*/*小结：从其他模块的视角看来，物理页帧分配的接口是调用 frame_alloc 函数得到一个 FrameTracker （如果物理内存还有剩余），它就代表了一个物理页帧，当它的生命周期结束之后它所控制的物理页帧将被自动回收。
### 多级页表实现
#### 页表基本数据结构与访问接口
SV39 多级页表是以节点为单位进行管理的。每个节点恰好存储在一个物理页帧中，它的位置可以用一个物理页号来表示。
```
 1// os/src/mm/page_table.rs
 2
 3pub struct PageTable {
 4    root_ppn: PhysPageNum,
 5    frames: Vec<FrameTracker>,
 6}
 7
 8impl PageTable {
 9    pub fn new() -> Self {
 10        let frame = frame_alloc().unwrap();
 11        PageTable {
 12            root_ppn: frame.ppn,
 13            frames: vec![frame],
 14        }
 15    }
 16}
```
每个应用的地址空间都对应一个不同的多级页表，这也就意味这不同页表的起始地址（即页表根节点的地址）是不一样的。 因此 PageTable 要保存它根节点的物理页号 root_ppn 作为页表唯一的区分标志。此外， 向量 frames 以 FrameTracker 的形式保存了页表所有的节点（包括根节点）所在的物理页帧。这与物理页帧管理模块 的测试程序是一个思路，即将这些 FrameTracker 的生命周期进一步绑定到 PageTable 下面。当 PageTable 生命周期结束后，向量 frames 里面的那些 FrameTracker 也会被回收，也就意味着存放多级页表节点的那些物理页帧 被回收了。

当通过 new 方法新建一个 PageTable 的时候，它只需有一个根节点。为此需要分配一个物理页帧 FrameTracker 并挂在向量 frames 下，然后更新根节点的物理页号 root_ppn 。

多级页表并不是被创建出来之后就不再变化的，为了 MMU 能够通过地址转换正确找到应用地址空间中的数据实际被内核放在内存中 位置，操作系统需要动态维护一个虚拟页号到页表项的映射，支持插入/删除键值对，其方法签名如下：
```
// os/src/mm/page_table.rs

impl PageTable {
    pub fn map(&mut self, vpn: VirtPageNum, ppn: PhysPageNum, flags: PTEFlags);
    pub fn unmap(&mut self, vpn: VirtPageNum);
}
```
通过 map 方法来在多级页表中插入一个键值对，注意这里将物理页号 ppn 和页表项标志位 flags 作为 不同的参数传入而不是整合为一个页表项；

相对的，通过 unmap 方法来删除一个键值对，在调用时仅需给出作为索引的虚拟页号即可。

在这些操作的过程中，需要访问或修改多级页表节点的内容。每个节点都被保存在一个物理页帧中，在多级页表的架构中，以 一个节点被存放在的物理页帧的物理页号作为指针指向该节点，这意味着，对于每个节点来说，一旦知道了指向它的物理页号,就能够修改这个节点的内容。

这就需要提前扩充多级页表维护的映射，使得对于每一个对应于某一特定物理页帧的物理页号 ppn ，均存在一个虚拟页号 vpn 能够映射到它，而且要能够较为简单的针对一个 ppn 找到某一个能映射到它的 vpn 。这里采用一种最 简单的 恒等映射 (Identical Mapping) ，也就是说对于物理内存上的每个物理页帧，都在多级页表中用一个与其 物理页号相等的虚拟页号映射到它。当想针对物理页号构造一个能映射到它的虚拟页号的时候，也只需使用一个和该物理页号 相等的虚拟页号即可。
#### 内核中访问物理页帧的方法
在内核中访问一个特定的物理页帧：
```
// os/src/mm/address.rs

impl PhysPageNum {
    pub fn get_pte_array(&self) -> &'static mut [PageTableEntry] {
        let pa: PhysAddr = self.clone().into();
        unsafe {
            core::slice::from_raw_parts_mut(pa.0 as *mut PageTableEntry, 512)
        }
    }
    pub fn get_bytes_array(&self) -> &'static mut [u8] {
        let pa: PhysAddr = self.clone().into();
        unsafe {
            core::slice::from_raw_parts_mut(pa.0 as *mut u8, 4096)
        }
    }
    pub fn get_mut<T>(&self) -> &'static mut T {
        let pa: PhysAddr = self.clone().into();
        unsafe {
            (pa.0 as *mut T).as_mut().unwrap()
        }
    }
}
```
构造可变引用来直接访问一个物理页号 PhysPageNum 对应的物理页帧，不同的引用类型对应于物理页帧上的一种不同的 内存布局，如 get_pte_array 返回的是一个页表项定长数组的可变引用，可以用来修改多级页表中的一个节点；而 get_bytes_array 返回的是一个字节数组的可变引用，可以以字节为粒度对物理页帧上的数据进行访问，前面进行数据清零 就用到了这个方法； get_mut 是个泛型函数，可以获取一个恰好放在一个物理页帧开头的类型为 T 的数据的可变引用。
在实现方面，都是先把物理页号转为物理地址 PhysAddr ，然后再转成 usize 形式的物理地址。接着，直接将它 转为裸指针用来访问物理地址指向的物理内存。在分页机制开启前，这样做自然成立；而开启之后，虽然裸指针被视为一个虚拟地址， 但是上面已经提到这种情况下虚拟地址会映射到一个相同的物理地址，因此在这种情况下也成立。注意，在返回值类型上附加了 静态生命周期泛型 'static ，这是为了绕过 Rust 编译器的借用检查，实质上可以将返回的类型也看成一个裸指针，因为 它也只是标识数据存放的位置以及类型。但与裸指针不同的是，无需通过 unsafe 的解引用访问它指向的数据，而是可以像一个 正常的可变引用一样直接访问。
#### 建立和拆除虚实地址映射关系
接下来介绍建立和拆除虚实地址映射关系的 map 和 unmap 方法是如何实现的。它们都依赖于一个很重要的过程， 也即在多级页表中找到一个虚拟地址对应的页表项。找到之后，只要修改页表项的内容即可完成键值对的插入和删除。 在寻找页表项的时候，可能出现页表的中间级节点还未被创建的情况，这个时候需要手动分配一个物理页帧来存放这个节点， 并将这个节点接入到当前的多级页表的某级中。
```
 1// os/src/mm/address.rs
 2
 3impl VirtPageNum {
 4    pub fn indexes(&self) -> [usize; 3] {
 5        let mut vpn = self.0;
 6        let mut idx = [0usize; 3];
 7        for i in (0..3).rev() {
 8            idx[i] = vpn & 511;
 9            vpn >>= 9;
 10        }
 11        idx
 12    }
 13}
 14
 15// os/src/mm/page_table.rs
 16
 17impl PageTable {
 18    fn find_pte_create(&mut self, vpn: VirtPageNum) -> Option<&mut PageTableEntry> {
 19        let idxs = vpn.indexes();
 20        let mut ppn = self.root_ppn;
 21        let mut result: Option<&mut PageTableEntry> = None;
 22        for i in 0..3 {
 23            let pte = &mut ppn.get_pte_array()[idxs[i]];
 24            if i == 2 {
 25                result = Some(pte);
 26                break;
 27            }
 28            if !pte.is_valid() {
 29                let frame = frame_alloc().unwrap();
 30                *pte = PageTableEntry::new(frame.ppn, PTEFlags::V);
 31                self.frames.push(frame);
 32            }
 33            ppn = pte.ppn();
 34        }
 35        result
 36    }
 37}
```
+ VirtPageNum 的 indexes 可以取出虚拟页号的三级页索引，并按照从高到低的顺序返回。注意它里面包裹的 usize 可能有 27 位，也有可能有 64-12=52 位，但这里我们是用来在多级页表上进行遍历，因此 只取出低 27 位。

+ PageTable::find_pte_create 在多级页表找到一个虚拟页号对应的页表项的可变引用方便后续的读写。如果在 遍历的过程中发现有节点尚未创建则会新建一个节点。

 变量 ppn 表示当前节点的物理页号，最开始指向多级页表的根节点。随后每次循环通过 get_pte_array 将 取出当前节点的页表项数组，并根据当前级页索引找到对应的页表项。如果当前节点是一个叶节点，那么直接返回这个页表项 的可变引用；否则尝试向下走。走不下去的话就新建一个节点，更新作为下级节点指针的页表项，并将新分配的物理页帧移动到 向量 frames 中方便后续的自动回收。注意在更新页表项的时候，不仅要更新物理页号，还要将标志位 V 置 1， 不然硬件在查多级页表的时候，会认为这个页表项不合法，从而触发 Page Fault 而不能向下走。

于是， map/unmap 就非常容易实现了：
```
// os/src/mm/page_table.rs

impl PageTable {
    pub fn map(&mut self, vpn: VirtPageNum, ppn: PhysPageNum, flags: PTEFlags) {
        let pte = self.find_pte_create(vpn).unwrap();
        assert!(!pte.is_valid(), "vpn {:?} is mapped before mapping", vpn);
        *pte = PageTableEntry::new(ppn, flags | PTEFlags::V);
    }
    pub fn unmap(&mut self, vpn: VirtPageNum) {
        let pte = self.find_pte_create(vpn).unwrap();
        assert!(pte.is_valid(), "vpn {:?} is invalid before unmapping", vpn);
        *pte = PageTableEntry::empty();
    }
}
```
只需根据虚拟页号找到页表项，然后修改或者直接清空其内容即可。
需要 PageTable 提供一种不经过 MMU 而是手动查页表的方法：
```
 1// os/src/mm/page_table.rs
 2
 3impl PageTable {
 4    /// Temporarily used to get arguments from user space.
 5    pub fn from_token(satp: usize) -> Self {
 6        Self {
 7            root_ppn: PhysPageNum::from(satp & ((1usize << 44) - 1)),
 8            frames: Vec::new(),
 9        }
 10    }
 11    fn find_pte(&self, vpn: VirtPageNum) -> Option<&PageTableEntry> {
 12        let idxs = vpn.indexes();
 13        let mut ppn = self.root_ppn;
 14        let mut result: Option<&PageTableEntry> = None;
 15        for i in 0..3 {
 16            let pte = &ppn.get_pte_array()[idxs[i]];
 17            if i == 2 {
 18                result = Some(pte);
 19                break;
 20            }
 21            if !pte.is_valid() {
 22                return None;
 23            }
 24            ppn = pte.ppn();
 25        }
 26        result
 27    }
 28    pub fn translate(&self, vpn: VirtPageNum) -> Option<PageTableEntry> {
 29        self.find_pte(vpn)
 30            .map(|pte| {pte.clone()})
 31    }
 32}
```
## 内核与应用的地址空间
通过基于页表的各种数据结构实现地址空间的抽象
### 实现地址空间抽象
#### 逻辑段：一段连续地址的虚拟内存
以逻辑段 MapArea 为单位描述一段连续地址的虚拟内存。所谓逻辑段，就是指地址区间中的一段实际可用（即 MMU 通过查多级页表 可以正确完成地址转换）的地址连续的虚拟地址区间，该区间内包含的所有虚拟页面都以一种相同的方式映射到物理页帧，具有可读/可写/可执行等属性。
```
// os/src/mm/memory_set.rs

pub struct MapArea {
    vpn_range: VPNRange,
    data_frames: BTreeMap<VirtPageNum, FrameTracker>,
    map_type: MapType,
    map_perm: MapPermission,
}
```
其中 VPNRange 描述一段虚拟页号的连续区间，表示该逻辑段在地址区间中的位置和长度。它是一个迭代器，可以使用 Rust 的语法糖 for-loop 进行迭代。
MapType 描述该逻辑段内的所有虚拟页面映射到物理页帧的同一种方式，它是一个枚举类型，在内核当前的实现中支持两种方式：
```
// os/src/mm/memory_set.rs

#[derive(Copy, Clone, PartialEq, Debug)]
pub enum MapType {
    Identical,
    Framed,
}
```
其中 Identical 表示之前也有提到的恒等映射，用于在启用多级页表之后仍能够访问一个特定的物理地址指向的物理内存；而 Framed 则表示对于每个虚拟页面都需要映射到一个新分配的物理页帧。

当逻辑段采用 MapType::Framed 方式映射到物理内存的时候， data_frames 是一个保存了该逻辑段内的每个虚拟页面 和它被映射到的物理页帧 FrameTracker 的一个键值对容器 BTreeMap 中，这些物理页帧被用来存放实际内存数据而不是 作为多级页表中的中间节点。和之前的 PageTable 一样，这也用到了 RAII 的思想，将这些物理页帧的生命周期绑定到它所在的逻辑段 MapArea 下，当逻辑段被回收之后这些之前分配的物理页帧也会自动地同时被回收。

MapPermission 表示控制该逻辑段的访问方式，它是页表项标志位 PTEFlags 的一个子集，仅保留 U/R/W/X 四个标志位，因为其他的标志位仅与硬件的地址转换机制细节相关，这样的设计能避免引入错误的标志位。
```
// os/src/mm/memory_set.rs

bitflags! {
    pub struct MapPermission: u8 {
        const R = 1 << 1;
        const W = 1 << 2;
        const X = 1 << 3;
        const U = 1 << 4;
    }
}
```
#### 地址空间：一系列有关联的逻辑段
地址空间是一系列有关联的逻辑段，这种关联一般是指这些逻辑段属于一个运行的程序（目前把一个运行的程序称为任务，后续会称为进程）。 用来表明正在运行的应用所在执行环境中的可访问内存空间，在这个内存空间中，包含了一系列的不一定连续的逻辑段。 这样我们就有任务的地址空间、内核的地址空间等说法了。地址空间使用 MemorySet 类型来表示：
```
// os/src/mm/memory_set.rs

pub struct MemorySet {
    page_table: PageTable,
    areas: Vec<MapArea>,
}
```
它包含了该地址空间的多级页表 page_table 和一个逻辑段 MapArea 的向量 areas 。注意 PageTable 下 挂着所有多级页表的节点所在的物理页帧，而每个 MapArea 下则挂着对应逻辑段中的数据所在的物理页帧，这两部分 合在一起构成了一个地址空间所需的所有物理页帧。这同样是一种 RAII 风格，当一个地址空间 MemorySet 生命周期结束后， 这些物理页帧都会被回收。

地址空间 MemorySet 的方法如下：
```
 1// os/src/mm/memory_set.rs
 2
 3impl MemorySet {
 4    pub fn new_bare() -> Self {
 5        Self {
 6            page_table: PageTable::new(),
 7            areas: Vec::new(),
 8        }
 9    }
 10    fn push(&mut self, mut map_area: MapArea, data: Option<&[u8]>) {
 11        map_area.map(&mut self.page_table);
 12        if let Some(data) = data {
 13            map_area.copy_data(&mut self.page_table, data);
 14        }
 15        self.areas.push(map_area);
 16    }
 17    /// Assume that no conflicts.
 18    pub fn insert_framed_area(
 19        &mut self,
 20        start_va: VirtAddr, end_va: VirtAddr, permission: MapPermission
 21    ) {
 22        self.push(MapArea::new(
 23            start_va,
 24            end_va,
 25            MapType::Framed,
 26            permission,
 27        ), None);
 28    }
 29    pub fn new_kernel() -> Self;
 30    /// Include sections in elf and trampoline and TrapContext and user stack,
 31    /// also returns user_sp and entry point.
 32    pub fn from_elf(elf_data: &[u8]) -> (Self, usize, usize);
 33}
```
在实现 push 方法在地址空间中插入一个逻辑段 MapArea 的时候，需要同时维护地址空间的多级页表 page_table 记录的虚拟页号到页表项的映射关系，也需要用到这个映射关系来找到向哪些物理页帧上拷贝初始数据。这用到了 MapArea 提供的另外几个方法：
```
 1// os/src/mm/memory_set.rs
 2
 3impl MapArea {
 4    pub fn new(
 5        start_va: VirtAddr,
 6        end_va: VirtAddr,
 7        map_type: MapType,
 8        map_perm: MapPermission
 9    ) -> Self {
 10        let start_vpn: VirtPageNum = start_va.floor();
 11        let end_vpn: VirtPageNum = end_va.ceil();
 12        Self {
 13            vpn_range: VPNRange::new(start_vpn, end_vpn),
 14            data_frames: BTreeMap::new(),
 15            map_type,
 16            map_perm,
 17        }
 18    }
 19    pub fn map(&mut self, page_table: &mut PageTable) {
 20        for vpn in self.vpn_range {
 21            self.map_one(page_table, vpn);
 22        }
 23    }
 24    pub fn unmap(&mut self, page_table: &mut PageTable) {
 25        for vpn in self.vpn_range {
 26            self.unmap_one(page_table, vpn);
 27        }
 28    }
 29    /// data: start-aligned but maybe with shorter length
 30    /// assume that all frames were cleared before
 31    pub fn copy_data(&mut self, page_table: &mut PageTable, data: &[u8]) {
 32        assert_eq!(self.map_type, MapType::Framed);
 33        let mut start: usize = 0;
 34        let mut current_vpn = self.vpn_range.get_start();
 35        let len = data.len();
 36        loop {
 37            let src = &data[start..len.min(start + PAGE_SIZE)];
 38            let dst = &mut page_table
 39                .translate(current_vpn)
 40                .unwrap()
 41                .ppn()
 42                .get_bytes_array()[..src.len()];
 43            dst.copy_from_slice(src);
 44            start += PAGE_SIZE;
 45            if start >= len {
 46                break;
 47            }
 48            current_vpn.step();
 49        }
 50    }
 51}
```
对逻辑段中的单个虚拟页面进行映射/解映射的方法 map_one 和 unmap_one 。显然它们的实现取决于当前 逻辑段被映射到物理内存的方式：
```
 1// os/src/mm/memory_set.rs
 2
 3impl MapArea {
 4    pub fn map_one(&mut self, page_table: &mut PageTable, vpn: VirtPageNum) {
 5        let ppn: PhysPageNum;
 6        match self.map_type {
 7            MapType::Identical => {
 8                ppn = PhysPageNum(vpn.0);
 9            }
 10            MapType::Framed => {
 11                let frame = frame_alloc().unwrap();
 12                ppn = frame.ppn;
 13                self.data_frames.insert(vpn, frame);
 14            }
 15        }
 16        let pte_flags = PTEFlags::from_bits(self.map_perm.bits).unwrap();
 17        page_table.map(vpn, ppn, pte_flags);
 18    }
 19    pub fn unmap_one(&mut self, page_table: &mut PageTable, vpn: VirtPageNum) {
 20        if self.map_type == MapType::Framed {
 21            self.data_frames.remove(&vpn);
 22        }
 23        page_table.unmap(vpn);
 24    }
 25}
```
### 内核地址空间
在本章之前，内核和应用代码的访存地址都被视为一个物理地址直接访问物理内存，而在分页模式开启之后，它们都需要通过 MMU 的 地址转换变成物理地址再交给 CPU 的访存单元去访问物理内存。地址空间抽象的重要意义在于 隔离 (Isolation) ，当我们 在执行每个应用的代码的时候，内核需要控制 MMU 使用这个应用地址空间的多级页表进行地址转换。由于每个应用地址空间在创建 的时候也顺带设置好了多级页表使得只有那些存放了它的数据的物理页帧能够通过该多级页表被映射到，这样它就只能访问自己的数据 而无法触及其他应用或是内核的数据。

启用分页模式下，内核代码的访存地址也会被视为一个虚拟地址并需要经过 MMU 的地址转换，因此我们也需要为内核对应构造一个 地址空间，它除了仍然需要允许内核的各数据段能够被正常访问之后，还需要包含所有应用的内核栈以及一个 跳板 (Trampoline) 。我们会在本章的最后一节再深入介绍跳板的机制。

下图是软件看到的 64 位地址空间在 SV39 分页模式下实际可能通过 MMU 检查的最高256GiB ：
![image-11](https://github.com/Kalining/code_repository/assets/148835940/5fc16be1-ce69-47f1-a75a-2051c718986e)

跳板放在最高的一个虚拟页面中。接下来则是从高到低放置每个应用的内核栈，内核栈的大小由 config 子模块的 KERNEL_STACK_SIZE 给出。它们的映射方式为 MapPermission 中的 rw 两个标志位，意味着这个逻辑段仅允许 CPU 处于内核态访问，且只能读或写。

注意相邻两个内核栈之间会预留一个 保护页面 (Guard Page) ，它是内核地址空间中的空洞，多级页表中并不存在与它相关的映射。 它的意义在于当内核栈空间不足（如调用层数过多或死递归）的时候，代码会尝试访问 空洞区域内的虚拟地址，然而它无法在多级页表中找到映射，便会触发异常，此时控制权会交给 trap handler 对这种情况进行 处理。由于编译器会对访存顺序和局部变量在栈帧中的位置进行优化，我们难以确定一个已经溢出的栈帧中的哪些位置会先被访问， 但总的来说，空洞区域被设置的越大，我们就能越早捕获到这一错误并避免它覆盖其他重要数据。由于我们的内核非常简单且内核栈 的大小设置比较宽裕，在当前的设计中我们仅将空洞区域的大小设置为单个页面。
内核地址空间的低 256GiB 的布局：
![image-12](https://github.com/Kalining/code_repository/assets/148835940/fad47edc-d318-486e-bef4-fd7bcb06305c)

四个逻辑段 .text/.rodata/.data/.bss 被恒等映射到物理内存，这使得在无需调整内核内存布局 os/src/linker.ld 的情况下就仍能和启用页表机制之前那样访问内核的各数据段。注意借用页表机制对这些逻辑段的访问方式做出了限制，这都是为了 在硬件的帮助下能够尽可能发现内核中的 bug ，在这里：

+ 四个逻辑段的 U 标志位均未被设置，使得 CPU 只能在处于 S 特权级（或以上）时访问它们；
+ 代码段 .text 不允许被修改；
+ 只读数据段 .rodata 不允许被修改，也不允许从它上面取指；
+ .data/.bss 均允许被读写，但是不允许从它上面取指。

此外， 之前 提到过内核地址空间中需要存在一个恒等映射到内核数据段之外的可用物理 页帧的逻辑段，这样才能在启用页表机制之后，内核仍能以纯软件的方式读写这些物理页帧。它们的标志位仅包含 rw ，意味着该 逻辑段只能在 S 特权级以上访问，并且只能读写。

创建内核地址空间的方法 new_kernel ：
```
 1// os/src/mm/memory_set.rs
 2
 3extern "C" {
 4    fn stext();
 5    fn etext();
 6    fn srodata();
 7    fn erodata();
 8    fn sdata();
 9    fn edata();
 10    fn sbss_with_stack();
 11    fn ebss();
 12    fn ekernel();
 13    fn strampoline();
 14}
 15
 16impl MemorySet {
 17    /// Without kernel stacks.
 18    pub fn new_kernel() -> Self {
 19        let mut memory_set = Self::new_bare();
 20        // map trampoline
 21        memory_set.map_trampoline();
 22        // map kernel sections
 23        println!(".text [{:#x}, {:#x})", stext as usize, etext as usize);
 24        println!(".rodata [{:#x}, {:#x})", srodata as usize, erodata as usize);
 25        println!(".data [{:#x}, {:#x})", sdata as usize, edata as usize);
 26        println!(".bss [{:#x}, {:#x})", sbss_with_stack as usize, ebss as usize);
 27        println!("mapping .text section");
 28        memory_set.push(MapArea::new(
 29            (stext as usize).into(),
 30            (etext as usize).into(),
 31            MapType::Identical,
 32            MapPermission::R | MapPermission::X,
 33        ), None);
 34        println!("mapping .rodata section");
 35        memory_set.push(MapArea::new(
 36            (srodata as usize).into(),
 37            (erodata as usize).into(),
 38            MapType::Identical,
 39            MapPermission::R,
 40        ), None);
 41        println!("mapping .data section");
 42        memory_set.push(MapArea::new(
 43            (sdata as usize).into(),
 44            (edata as usize).into(),
 45            MapType::Identical,
 46            MapPermission::R | MapPermission::W,
 47        ), None);
 48        println!("mapping .bss section");
 49        memory_set.push(MapArea::new(
 50            (sbss_with_stack as usize).into(),
 51            (ebss as usize).into(),
 52            MapType::Identical,
 53            MapPermission::R | MapPermission::W,
 54        ), None);
 55        println!("mapping physical memory");
 56        memory_set.push(MapArea::new(
 57            (ekernel as usize).into(),
 58            MEMORY_END.into(),
 59            MapType::Identical,
 60            MapPermission::R | MapPermission::W,
 61        ), None);
 62        memory_set
 63    }
 64}
new_kernel 将映射跳板和地址空间中最低 256GiB 中的所有的逻辑段。
### 应用地址空间
借助地址空间的抽象，可以让所有应用程序都使用同样的起始地址，所有应用可以使用同一个链接脚本了：
```
 1/* user/src/linker.ld */
 2
 3OUTPUT_ARCH(riscv)
 4ENTRY(_start)
 5
 6BASE_ADDRESS = 0x0;
 7
 8SECTIONS
 9{
 10    . = BASE_ADDRESS;
 11    .text : {
 12        *(.text.entry)
 13        *(.text .text.*)
 14    }
 15    . = ALIGN(4K);
 16    .rodata : {
 17        *(.rodata .rodata.*)
 18    }
 19    . = ALIGN(4K);
 20    .data : {
 21        *(.data .data.*)
 22    }
 23    .bss : {
 24        *(.bss .bss.*)
 25    }
 26    /DISCARD/ : {
 27        *(.eh_frame)
 28        *(.debug*)
 29    }
 30}
将起始地址 BASE_ADDRESS 设置为 0x0 ，显然它只能是一个地址空间中的虚拟地址而非物理地址。 事实上由于我们将入口汇编代码段放在最低的地方，这也是整个应用的入口点。此外，在 .text 和 .rodata 中间以及 .rodata 和 .data 中间我们进行了页面对齐，因为前后两个逻辑段的访问方式限制是不同的，由于只能以页为单位对这个限制进行设置， 因此就只能将下一个逻辑段对齐到下一个页面开始放置。相对的， .data 和 .bss 两个逻辑段由于限制相同，它们中间 则无需进行页面对齐。
应用地址空间的布局：
![image-13](https://github.com/Kalining/code_repository/assets/148835940/e3cf06b9-3721-47cc-9f9f-cfd9a6259581)

左侧给出了应用地址空间最低 256GiB 的布局：从 0x0 开始向高地址放置应用内存布局中的 各个逻辑段，最后放置带有一个保护页面的用户栈。这些逻辑段都是以 Framed 方式映射到物理内存的，从访问方式上来说都加上 了 U 标志位代表 CPU 可以在 U 特权级也就是执行应用代码的时候访问它们。右侧则给出了最高的 256GiB ， 可以看出它只是和内核地址空间一样将跳板放置在最高页，还将 Trap 上下文放置在次高页中。这两个虚拟页面虽然位于应用地址空间， 但是它们并不包含 U 标志位，事实上它们在地址空间切换的时候才会发挥作用。

在 os/src/build.rs 中，不再将丢弃了所有符号的应用二进制镜像链接进内核，而是直接使用 ELF 格式的可执行文件， 因为在前者中内存布局中各个逻辑段的位置和访问限制等信息都被裁剪掉了。而 loader 子模块也变得精简：
```
// os/src/loader.rs

pub fn get_num_app() -> usize {
    extern "C" { fn _num_app(); }
    unsafe { (_num_app as usize as *const usize).read_volatile() }
}

pub fn get_app_data(app_id: usize) -> &'static [u8] {
    extern "C" { fn _num_app(); }
    let num_app_ptr = _num_app as usize as *const usize;
    let num_app = get_num_app();
    let app_start = unsafe {
        core::slice::from_raw_parts(num_app_ptr.add(1), num_app + 1)
    };
    assert!(app_id < num_app);
    unsafe {
        core::slice::from_raw_parts(
            app_start[app_id] as *const u8,
            app_start[app_id + 1] - app_start[app_id]
        )
    }
}
```
它仅需要提供两个函数： get_num_app 获取链接到内核内的应用的数目，而 get_app_data 则根据传入的应用编号 取出对应应用的 ELF 格式可执行文件数据。它们和之前一样仍是基于 build.rs 生成的 link_app.S 给出的符号来 确定其位置，并实际放在内核的数据段中。 loader 模块中原有的内核和用户栈则分别作为逻辑段放在内核和用户地址空间中，我们无需再去专门为其定义一种类型。

在创建应用地址空间的时候，需要对 get_app_data 得到的 ELF 格式数据进行解析，找到各个逻辑段所在位置和访问 限制并插入进来，最终得到一个完整的应用地址空间：
```
 1// os/src/mm/memory_set.rs
 2
 3impl MemorySet {
 4    /// Include sections in elf and trampoline and TrapContext and user stack,
 5    /// also returns user_sp and entry point.
 6    pub fn from_elf(elf_data: &[u8]) -> (Self, usize, usize) {
 7        let mut memory_set = Self::new_bare();
 8        // map trampoline
 9        memory_set.map_trampoline();
 10        // map program headers of elf, with U flag
 11        let elf = xmas_elf::ElfFile::new(elf_data).unwrap();
 12        let elf_header = elf.header;
 13        let magic = elf_header.pt1.magic;
 14        assert_eq!(magic, [0x7f, 0x45, 0x4c, 0x46], "invalid elf!");
 15        let ph_count = elf_header.pt2.ph_count();
 16        let mut max_end_vpn = VirtPageNum(0);
 17        for i in 0..ph_count {
 18            let ph = elf.program_header(i).unwrap();
 19            if ph.get_type().unwrap() == xmas_elf::program::Type::Load {
 20                let start_va: VirtAddr = (ph.virtual_addr() as usize).into();
 21                let end_va: VirtAddr = ((ph.virtual_addr() + ph.mem_size()) as usize).into();
 22                let mut map_perm = MapPermission::U;
 23                let ph_flags = ph.flags();
 24                if ph_flags.is_read() { map_perm |= MapPermission::R; }
 25                if ph_flags.is_write() { map_perm |= MapPermission::W; }
 26                if ph_flags.is_execute() { map_perm |= MapPermission::X; }
 27                let map_area = MapArea::new(
 28                    start_va,
 29                    end_va,
 30                    MapType::Framed,
 31                    map_perm,
 32                );
 33                max_end_vpn = map_area.vpn_range.get_end();
 34                memory_set.push(
 35                    map_area,
 36                    Some(&elf.input[ph.offset() as usize..(ph.offset() + ph.file_size()) as usize])
 37                );
 38            }
 39        }
 40        // map user stack with U flags
 41        let max_end_va: VirtAddr = max_end_vpn.into();
 42        let mut user_stack_bottom: usize = max_end_va.into();
 43        // guard page
 44        user_stack_bottom += PAGE_SIZE;
 45        let user_stack_top = user_stack_bottom + USER_STACK_SIZE;
 46        memory_set.push(MapArea::new(
 47            user_stack_bottom.into(),
 48            user_stack_top.into(),
 49            MapType::Framed,
 50            MapPermission::R | MapPermission::W | MapPermission::U,
 51        ), None);
 52        // map TrapContext
 53        memory_set.push(MapArea::new(
 54            TRAP_CONTEXT.into(),
 55            TRAMPOLINE.into(),
 56            MapType::Framed,
 57            MapPermission::R | MapPermission::W,
 58        ), None);
 59        (memory_set, user_stack_top, elf.header.pt2.entry_point() as usize)
 60    }
 61}

## 基于地址空间的分时多任务
### 建立并开启基于分页模式的虚拟地址空间
当 SBI 实现（本项目中基于 RustSBI）初始化完成后， CPU 将跳转到内核入口点并在 S 特权级上执行，此时还并没有开启分页模式 ，内核的每一次访存仍被视为一个物理地址直接访问物理内存。而在开启分页模式之后，内核的代码在访存的时候只能看到内核地址空间， 此时每次访存将被视为一个虚拟地址且需要通过 MMU 基于内核地址空间的多级页表的地址转换。这两种模式之间的过渡在内核初始化期间 完成。
#### 创建内核地址空间
创建内核地址空间的全局实例：
```
// os/src/mm/memory_set.rs

lazy_static! {
    pub static ref KERNEL_SPACE: Arc<UPSafeCell<MemorySet>> = Arc::new(unsafe {
        UPSafeCell::new(MemorySet::new_kernel()
    )});
}
```
从之前对于 lazy_static! 宏的介绍可知， KERNEL_SPACE 在运行期间它第一次被用到时才会实际进行初始化，而它所 占据的空间则是编译期被放在全局数据段中。 Arc<UPSafeCell<_>> 同时带来 Arc<T> 提供的共享 引用，和 UPSafeCell<T> 提供的互斥访问。

在 rust_main 函数中，首先调用 mm::init 进行内存管理子系统的初始化：
```
// os/src/mm/mod.rs

pub use memory_set::KERNEL_SPACE;

pub fn init() {
    heap_allocator::init_heap();
    frame_allocator::init_frame_allocator();
    KERNEL_SPACE.exclusive_access().activate();
}
```
最先进行了全局动态内存分配器的初始化(接下来马上就要用到 Rust 的堆数据结构)。接下来初始化物理页帧 管理器（内含堆数据结构 Vec<T> ）使能可用物理页帧的分配和回收能力。最后创建内核地址空间并让 CPU 开启分页模式， MMU 在地址转换的时候使用内核的多级页表，这一切均在一行之内做到：
+ 首先，我们引用 KERNEL_SPACE ，这是它第一次被使用，就在此时它会被初始化，调用 MemorySet::new_kernel 创建一个内核地址空间并使用 Arc<UPSafeCell<T>> 包裹起来；
+ 最然后，我们调用 MemorySet::activate ：
```
 1// os/src/mm/page_table.rs
 2
 3pub fn token(&self) -> usize {
 4    8usize << 60 | self.root_ppn.0
 5}
 6
 7// os/src/mm/memory_set.rs
 8
 9impl MemorySet {
 10    pub fn activate(&self) {
 11        let satp = self.page_table.token();
 12        unsafe {
 13            satp::write(satp);
 14            core::arch::asm!("sfence.vma");
 15        }
 16    }
 17}
PageTable::token 会按照 satp CSR 格式要求 构造一个无符号 64 位无符号整数，使得其 分页模式为 SV39 ，且将当前多级页表的根节点所在的物理页号填充进去。在 activate 中，我们将这个值写入当前 CPU 的 satp CSR ，从这一刻开始 SV39 分页模式就被启用了，而且 MMU 会使用内核地址空间的多级页表进行地址转换。

必须注意切换 satp CSR 是否是一个 平滑 的过渡：其含义是指，切换 satp 的指令及其下一条指令这两条相邻的指令的 虚拟地址是相邻的（由于切换 satp 的指令并不是一条跳转指令， pc 只是简单的自增当前指令的字长）， 而它们所在的物理地址一般情况下也是相邻的，但是它们所经过的地址转换流程却是不同的——切换 satp 导致 MMU 查的多级页表 是不同的。这就要求前后两个地址空间在切换 satp 的指令 附近 的映射满足某种意义上的连续性。

这条写入 satp 的指令及其下一条指令都在内核内存布局的代码段中，在切换之后是一个恒等映射， 而在切换之前是视为物理地址直接取指，也可以将其看成一个恒等映射。这完全符合我们的期待：即使切换了地址空间，指令仍应该 能够被连续的执行。

注意到在 activate 的最后，插入了一条汇编指令 sfence.vma 

回顾一下多级页表：它相比线性表虽然大量节约了内存占用，但是却需要 MMU 进行更多的隐式访存。如果是一个线性表， MMU 仅需单次访存就能找到页表项并完成地址转换，而多级页表（以 SV39 为例，不考虑大页）最顺利的情况下也需要三次访存。这些 额外的访存和真正访问数据的那些访存在空间上并不相邻，加大了多级缓存的压力，一旦缓存缺失将带来巨大的性能惩罚。如果采用 多级页表实现，这个问题会变得更为严重，使得地址空间抽象的性能开销过大。

为了解决性能问题，一种常见的做法是在 CPU 中利用部分硬件资源额外加入一个 快表 (TLB, Translation Lookaside Buffer) ， 它维护了部分虚拟页号到页表项的键值对。当 MMU 进行地址转换的时候，首先 会到快表中看看是否匹配，如果匹配的话直接取出页表项完成地址转换而无需访存；否则再去查页表并将键值对保存在快表中。一旦 我们修改了 satp 切换了地址空间，快表中的键值对就会失效，因为它还表示着上个地址空间的映射关系。为了 MMU 的地址转换 能够及时与 satp 的修改同步，我们可以选择立即使用 sfence.vma 指令将快表清空，这样 MMU 就不会看到快表中已经 过期的键值对了。
#### 跳板的实现
当 __alltraps 保存 Trap 上下文的时候，必须通过修改 satp 从应用地址空间切换到内核地址空间， 因为 trap handler 只有在内核地址空间中才能访问； 同理，在 __restore 恢复 Trap 上下文的时候，也必须从内核地址空间切换回应用地址空间，因为应用的代码和 数据只能在它自己的地址空间中才能访问，内核地址空间是看不到的。 进而，地址空间的切换不能影响指令的连续执行，这就要求应用和内核地址空间在切换地址空间指令附近是平滑的。   
在 Trap 上下文中包含更多内容（它们在初始化之后便只会被读取而不会被写入 ，并不是每次都需要保存/恢复）：
```
 1// os/src/trap/context.rs
 2
 3#[repr(C)]
 4pub struct TrapContext {
 5    pub x: [usize; 32],
 6    pub sstatus: Sstatus,
 7    pub sepc: usize,
 8    pub kernel_satp: usize,
 9    pub kernel_sp: usize,
 10    pub trap_handler: usize,
 11}
```
在多出的三个字段中：
+ kernel_satp 表示内核地址空间的 token ；
+ kernel_sp 表示当前应用在内核地址空间中的内核栈栈顶的虚拟地址；
+ trap_handler 表示内核中 trap handler 入口点的虚拟地址。

它们在应用初始化的时候由内核写入应用地址空间中的 TrapContext 的相应位置，此后就不再被修改。

现在的 __alltraps 和 __restore 各是如何在保存和恢复 Trap 上下文的同时也切换地址空间的：
```
 1# os/src/trap/trap.S
 2
 3    .section .text.trampoline
 4    .globl __alltraps
 5    .globl __restore
 6    .align 2
 7__alltraps:
 8    csrrw sp, sscratch, sp
 9    # now sp->*TrapContext in user space, sscratch->user stack
10    # save other general purpose registers
11    sd x1, 1*8(sp)
12    # skip sp(x2), we will save it later
13    sd x3, 3*8(sp)
14    # skip tp(x4), application does not use it
15    # save x5~x31
16    .set n, 5
17    .rept 27
18        SAVE_GP %n
19        .set n, n+1
20    .endr
21    # we can use t0/t1/t2 freely, because they have been saved in TrapContext
22    csrr t0, sstatus
23    csrr t1, sepc
24    sd t0, 32*8(sp)
25    sd t1, 33*8(sp)
26    # read user stack from sscratch and save it in TrapContext
27    csrr t2, sscratch
28    sd t2, 2*8(sp)
29    # load kernel_satp into t0
30    ld t0, 34*8(sp)
31    # load trap_handler into t1
32    ld t1, 36*8(sp)
33    # move to kernel_sp
34    ld sp, 35*8(sp)
35    # switch to kernel space
36    csrw satp, t0
37    sfence.vma
38    # jump to trap_handler
39    jr t1
40
41__restore:
42    # a0: *TrapContext in user space(Constant); a1: user space token
43    # switch to user space
44    csrw satp, a1
45    sfence.vma
46    csrw sscratch, a0
47    mv sp, a0
48    # now sp points to TrapContext in user space, start restoring based on it
49    # restore sstatus/sepc
50    ld t0, 32*8(sp)
51    ld t1, 33*8(sp)
52    csrw sstatus, t0
53    csrw sepc, t1
54    # restore general purpose registers except x0/sp/tp
55    ld x1, 1*8(sp)
56    ld x3, 3*8(sp)
57    .set n, 5
58    .rept 27
59        LOAD_GP %n
60        .set n, n+1
61    .endr
62    # back to user stack
63    ld sp, 2*8(sp)
64    sret
```
当应用 Trap 进入内核的时候，硬件会设置一些 CSR 并在 S 特权级下跳转到 __alltraps 保存 Trap 上下文。此时 sp 寄存器仍指向用户栈，但 sscratch 则被设置为指向应用地址空间中存放 Trap 上下文的位置，实际在次高页面。 随后，就像之前一样，我们 csrrw 交换 sp 和 sscratch ，并基于指向 Trap 上下文位置的 sp 开始保存通用 寄存器和一些 CSR ，这个过程在第 28 行结束。到这里，我们就全程在应用地址空间中完成了保存 Trap 上下文的工作。

接下来该考虑切换到内核地址空间并跳转到 trap handler 了。第 30 行将内核地址空间的 token 载入到 t0 寄存器中， 第 32 行将 trap handler 入口点的虚拟地址载入到 t1 寄存器中，第 34 行直接将 sp 修改为应用内核栈顶的地址。 这三条信息均是内核在初始化该应用的时候就已经设置好的。第 36~37 行将 satp 修改为内核地址空间的 token 并使用 sfence.vma 刷新快表，这就切换到了内核地址空间。最后在第 39 行通过 jr 指令跳转到 t1 寄存器所保存的 trap handler 入口点的地址。注意这里不能像之前的章节那样直接 call trap_handler。

当内核将 Trap 处理完毕准备返回用户态的时候会 调用 __restore ，它有两个参数：第一个是 Trap 上下文在应用 地址空间中的位置，这个对于所有的应用来说都是相同的，由调用规范在 a0 寄存器中传递；第二个则是即将回到的应用的地址空间 的 token ，在 a1 寄存器中传递。由于 Trap 上下文是保存在应用地址空间中的，第 44~45 行先切换回应用地址空间。第 46 行将传入的 Trap 上下文位置保存在 sscratch 寄存器中，这样 __alltraps 中才能基于它将 Trap 上下文 保存到正确的位置。第 47 行将 sp 修改为 Trap 上下文的位置，后面基于它恢复各通用寄存器和 CSR。最后在第 64 行， 通过 sret 指令返回用户态。

接下来还需要考虑切换地址空间前后指令能否仍能连续执行。可以看到将 trap.S 中的整段汇编代码放置在 .text.trampoline 段，并在调整内存布局的时候将它对齐到代码段的一个页面中：
```
 1# os/src/linker.ld
 2
 3    stext = .;
 4    .text : {
 5        *(.text.entry)
 6+        . = ALIGN(4K);
 7+        strampoline = .;
 8+        *(.text.trampoline);
 9+        . = ALIGN(4K);
10        *(.text .text.*)
11    }
```
这段汇编代码放在一个物理页帧中，且 __alltraps 恰好位于这个物理页帧的开头，其物理地址被外部符号 strampoline 标记。在开启分页模式之后，内核和应用代码都只能看到各自的虚拟地址空间，而在它们的视角中，这段汇编代码 被放在它们地址空间的最高虚拟页面上，由于这段汇编代码在执行的时候涉及到地址空间切换，故而被称为跳板页面。

那么在产生trap前后的一小段时间内会有一个比较 极端 的情况，即刚产生trap时，CPU已经进入了内核态（即Supervisor Mode）， 但此时执行代码和访问数据还是在应用程序所处的用户态虚拟地址空间中，而不是我们通常理解的内核虚拟地址空间。在这段特殊的时间内，CPU指令 为什么能够被连续执行呢？这里需要注意：无论是内核还是应用的地址空间，跳板的虚拟页均位于同样位置，且它们也将会映射到同一个实际存放这段 汇编代码的物理页帧。也就是说，在执行 __alltraps 或 __restore 函数进行地址空间切换的时候， 应用的用户态虚拟地址空间和操作系统内核的内核态虚拟地址空间对切换地址空间的指令所在页的映射方式均是相同的， 这就说明了这段切换地址空间的指令控制流仍是可以连续执行的。

现在可以说明我们在创建用户/内核地址空间中用到的 map_trampoline 是如何实现的了：
```
 1// os/src/config.rs
 2
 3pub const TRAMPOLINE: usize = usize::MAX - PAGE_SIZE + 1;
 4
 5// os/src/mm/memory_set.rs
 6
 7impl MemorySet {
 8    /// Mention that trampoline is not collected by areas.
 9    fn map_trampoline(&mut self) {
10        self.page_table.map(
11            VirtAddr::from(TRAMPOLINE).into(),
12            PhysAddr::from(strampoline as usize).into(),
13            PTEFlags::R | PTEFlags::X,
14        );
15    }
16}
```
这里为了实现方便并没有新增逻辑段 MemoryArea 而是直接在多级页表中插入一个从地址空间的最高虚拟页面映射到 跳板汇编代码所在的物理页帧的键值对，访问方式限制与代码段相同，即 RX 。

在 __alltraps 中需要借助寄存器 jr 而不能直接 call trap_handler :    
因为在 内存布局中，这条 .text.trampoline 段中的跳转指令和 trap_handler 都在代码段之内，汇编器（Assembler） 和链接器（Linker）会根据 linker.ld 的地址布局描述，设定电子指令的地址，并计算二者地址偏移量 并让跳转指令的实际效果为当前 pc 自增这个偏移量。但实际上这条跳转指令在被执行的时候， 它的虚拟地址被操作系统内核设置在地址空间中的最高页面之内，加上这个偏移量并不能正确的得到 trap_handler 的入口地址。

问题的本质可以概括为：跳转指令实际被执行时的虚拟地址和在编译器/汇编器/链接器进行后端代码生成和链接形成最终机器码时设置此指令的地址是不同的。
### 加载和执行应用程序
#### 扩展任务控制块
为了让应用在运行时有一个安全隔离且符合编译器给应用设定的地址空间布局的虚拟地址空间，操作系统需要对任务进行更多的管理，所以任务控制块相比第三章也包含了更多内容：
```
1// os/src/task/task.rs
2
3pub struct TaskControlBlock {
4    pub task_status: TaskStatus,
5    pub task_cx: TaskContext,
6    pub memory_set: MemorySet,
7    pub trap_cx_ppn: PhysPageNum,
8    pub base_size: usize,
9}
```
除了应用的地址空间 memory_set 之外，还有位于应用地址空间次高页的 Trap 上下文被实际存放在物理页帧的物理页号 trap_cx_ppn ，它能够方便我们对于 Trap 上下文进行访问。此外， base_size 统计了应用数据的大小，也就是 在应用地址空间中从 
 开始到用户栈结束一共包含多少字节。它后续还应该包含用于应用动态内存分配的 堆空间的大小，但我们暂不支持。
#### 更新对任务控制块的管理
任务控制块的创建：
```
 1// os/src/config.rs
 2
 3/// Return (bottom, top) of a kernel stack in kernel space.
 4pub fn kernel_stack_position(app_id: usize) -> (usize, usize) {
 5    let top = TRAMPOLINE - app_id * (KERNEL_STACK_SIZE + PAGE_SIZE);
 6    let bottom = top - KERNEL_STACK_SIZE;
 7    (bottom, top)
 8}
 9
10// os/src/task/task.rs
11
12impl TaskControlBlock {
13    pub fn new(elf_data: &[u8], app_id: usize) -> Self {
14        // memory_set with elf program headers/trampoline/trap context/user stack
15        let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data);
16        let trap_cx_ppn = memory_set
17            .translate(VirtAddr::from(TRAP_CONTEXT).into())
18            .unwrap()
19            .ppn();
20        let task_status = TaskStatus::Ready;
21        // map a kernel-stack in kernel space
22        let (kernel_stack_bottom, kernel_stack_top) = kernel_stack_position(app_id);
23        KERNEL_SPACE
24            .exclusive_access()
25            .insert_framed_area(
26                kernel_stack_bottom.into(),
27                kernel_stack_top.into(),
28                MapPermission::R | MapPermission::W,
29        );
30        let task_control_block = Self {
31            task_status,
32            task_cx: TaskContext::goto_trap_return(kernel_stack_top),
33            memory_set,
34            trap_cx_ppn,
35            base_size: user_sp,
36            heap_bottom: user_sp,
37            program_brk: user_sp,
38        };
39        // prepare TrapContext in user space
40        let trap_cx = task_control_block.get_trap_cx();
41        *trap_cx = TrapContext::app_init_context(
42            entry_point,
43            user_sp,
44            KERNEL_SPACE.exclusive_access().token(),
45            kernel_stack_top,
46            trap_handler as usize,
47        );
48        task_control_block
49    }
50}
```
初始化该应用的 Trap 上下文，由于它是在应用地址空间而不是在内核地址空间中，只能手动查页表找到 Trap 上下文实际被放在的物理页帧，再获得在用户空间的 Trap 上下文的可变引用用于初始化：
```
// os/src/task/task.rs

impl TaskControlBlock {
    pub fn get_trap_cx(&self) -> &'static mut TrapContext {
        self.trap_cx_ppn.get_mut()
    }
}
```
返回 'static 的可变引用和之前一样可以看成一个绕过 unsafe 的裸指针；而 PhysPageNum::get_mut 是一个泛型函数，由于我们已经声明了总体返回 TrapContext 的可变引用，则Rust编译器会给 get_mut 泛型函数针对具体类型 TrapContext 的情况生成一个特定版本的 get_mut 函数实现。在 get_trap_cx 函数中则会静态调用``get_mut`` 泛型函数的特定版本实现。
```
 1// os/src/trap/context.rs
 2
 3impl TrapContext {
 4    pub fn set_sp(&mut self, sp: usize) { self.x[2] = sp; }
 5    pub fn app_init_context(
 6        entry: usize,
 7        sp: usize,
 8        kernel_satp: usize,
 9        kernel_sp: usize,
10        trap_handler: usize,
11    ) -> Self {
12        let mut sstatus = sstatus::read();
13        sstatus.set_spp(SPP::User);
14        let mut cx = Self {
15            x: [0; 32],
16            sstatus,
17            sepc: entry,
18            kernel_satp,
19            kernel_sp,
20            trap_handler,
21        };
22        cx.set_sp(sp);
23        cx
24    }
25}
```
和之前相比 TrapContext::app_init_context 需要补充上让应用在 __alltraps 能够顺利进入到内核地址空间 并跳转到 trap handler 入口点的相关信息。

在内核初始化的时候，需要将所有的应用加载到全局应用管理器中：
```
 1// os/src/task/mod.rs
 2
 3struct TaskManagerInner {
 4    tasks: Vec<TaskControlBlock>,
 5    current_task: usize,
 6}
 7
 8lazy_static! {
 9    pub static ref TASK_MANAGER: TaskManager = {
10        info!("init TASK_MANAGER");
11        let num_app = get_num_app();
12        info!("num_app = {}", num_app);
13        let mut tasks: Vec<TaskControlBlock> = Vec::new();
14        for i in 0..num_app {
15            tasks.push(TaskControlBlock::new(get_app_data(i), i));
16        }
17        TaskManager {
18            num_app,
19            inner: unsafe {
20                UPSafeCell::new(TaskManagerInner {
21                    tasks,
22                    current_task: 0,
23                })
24            },
25        }
26    };
27}
```
在 TaskManagerInner 中使用向量 Vec 来保存任务控制块。在全局任务管理器 TASK_MANAGER 初始化的时候，只需使用 loader 子模块提供的 get_num_app 和 get_app_data 分别获取链接到内核的应用 数量和每个应用的 ELF 文件格式的数据，然后依次给每个应用创建任务控制块并加入到向量中即可。还将 current_task 设置 为 0 ，于是将从第 0 个应用开始执行。

应用构建器 os/build.rs 的改动：

首先在 .incbin 中不再插入清除全部符号的应用二进制镜像 *.bin ，而是将构建得到的 ELF 格式文件直接链接进来；

其次在链接每个 ELF 格式文件之前都加入一行 .align 3 来确保它们对齐到 8 字节，这是由于如果不这样做， xmas-elf crate 可能会在解析 ELF 的时候进行不对齐的内存读写，例如使用 ld 指令从内存的一个没有对齐到 8 字节的地址加载一个 64 位的值到一个通用寄存器。

为了方便后续的实现，全局任务管理器还需要提供关于当前应用与地址空间有关的一些信息。通过 current_user_token 和 current_trap_cx 分别可以获得当前正在执行的应用的地址空间的 token 和可以在 内核地址空间中修改位于该应用地址空间中的 Trap 上下文的可变引用。
### 改进 Trap 处理的实现
为了能够支持地址空间， trap_handler 的改进实现：
```
 1// os/src/trap/mod.rs
 2
 3fn set_kernel_trap_entry() {
 4    unsafe {
 5        stvec::write(trap_from_kernel as usize, TrapMode::Direct);
 6    }
 7}
 8
 9#[no_mangle]
10pub fn trap_from_kernel() -> ! {
11    panic!("a trap from kernel!");
12}
13
14#[no_mangle]
15pub fn trap_handler() -> ! {
16    set_kernel_trap_entry();
17    let cx = current_trap_cx();
18    let scause = scause::read();
19    let stval = stval::read();
20    match scause.cause() {
21        ...
22    }
23    trap_return();
24}
```
由于应用的 Trap 上下文不在内核地址空间，因此调用 current_trap_cx 来获取当前应用的 Trap 上下文的可变引用 而不是像之前那样作为参数传入 trap_handler 。至于 Trap 处理的过程则没有发生什么变化。

注意到，在 trap_handler 的开头还调用 set_kernel_trap_entry 将 stvec 修改为同模块下另一个函数 trap_from_kernel 的地址。这就是说，一旦进入内核后再次触发到 S 的 Trap，则会在硬件设置一些 CSR 之后跳过寄存器 的保存过程直接跳转到 trap_from_kernel 函数，在这里我们直接 panic 退出。这是因为内核和应用的地址空间分离 之后，从 U 还是从 S Trap 到 S 的 Trap 上下文保存与恢复实现方式和 Trap 处理逻辑有很大差别，我们不得不实现两遍而 不太可能将二者整合起来。这里简单起见我们弱化了从 S 到 S 的 Trap ，省略了 Trap 上下文保存过程而直接 panic 。

在 trap_handler 完成 Trap 处理之后，需要调用 trap_return 返回用户态：
```
 1// os/src/trap/mod.rs
 2
 3fn set_user_trap_entry() {
 4    unsafe {
 5        stvec::write(TRAMPOLINE as usize, TrapMode::Direct);
 6    }
 7}
 8
 9#[no_mangle]
10pub fn trap_return() -> ! {
11    set_user_trap_entry();
12    let trap_cx_ptr = TRAP_CONTEXT;
13    let user_satp = current_user_token();
14    extern "C" {
15        fn __alltraps();
16        fn __restore();
17    }
18    let restore_va = __restore as usize - __alltraps as usize + TRAMPOLINE;
19    unsafe {
20            core::arch::asm!(
21            "fence.i",
22            "jr {restore_va}",
23            restore_va = in(reg) restore_va,
24            in("a0") trap_cx_ptr,
25            in("a1") user_satp,
26            options(noreturn)
27        );
28    }
29    panic!("Unreachable in back_to_user!");
30}
```
### 改进 sys_write 的实现
由于内核和应用地址空间的隔离， sys_write 不再能够直接访问位于应用空间中的数据，而需要手动查页表才能知道那些 数据被放置在哪些物理页帧上并进行访问。

为此，页表模块 page_table 提供了将应用地址空间中一个缓冲区转化为在内核空间中能够直接访问的形式的辅助函数：
```
 1// os/src/mm/page_table.rs
 2
 3pub fn translated_byte_buffer(
 4    token: usize,
 5    ptr: *const u8,
 6    len: usize
 7) -> Vec<&'static [u8]> {
 8    let page_table = PageTable::from_token(token);
 9    let mut start = ptr as usize;
10    let end = start + len;
11    let mut v = Vec::new();
12    while start < end {
13        let start_va = VirtAddr::from(start);
14        let mut vpn = start_va.floor();
15        let ppn = page_table
16            .translate(vpn)
17            .unwrap()
18            .ppn();
19        vpn.step();
20        let mut end_va: VirtAddr = vpn.into();
21        end_va = end_va.min(VirtAddr::from(end));
22        v.push(&ppn.get_bytes_array()[start_va.page_offset()..end_va.page_offset()]);
23        start = end_va.into();
24    }
25    v
26}
```
参数中的 token 是某个应用地址空间的 token ， ptr 和 len 则分别表示该地址空间中的一段缓冲区的起始地址 和长度。 translated_byte_buffer 会以向量的形式返回一组可以在内核空间中直接访问的字节数组切片。

进而，对 sys_write 系统调用的改造：
```
// os/src/syscall/fs.rs

pub fn sys_write(fd: usize, buf: *const u8, len: usize) -> isize {
    match fd {
        FD_STDOUT => {
            let buffers = translated_byte_buffer(current_user_token(), buf, len);
            for buffer in buffers {
                print!("{}", core::str::from_utf8(buffer).unwrap());
            }
            len as isize
        },
        _ => {
            panic!("Unsupported fd in sys_write!");
        }
    }
}
```
将每个字节数组切片转化为字符串 &str 然后输出。

## 作业
1. 请列举 SV39 页表页表项的组成，描述其中的标志位有何作用？

2. 缺页
缺页指的是进程访问页面时页面不在页表中或在页表中无效的现象，此时 MMU 将会返回一个中断， 告知 os 进程内存访问出了问题。os 选择填补页表并重新执行异常指令或者杀死进程。

+ 请问哪些异常可能是缺页导致的？

+ 发生缺页时，描述相关重要寄存器的值，上次实验描述过的可以简略。

缺页有两个常见的原因，其一是 Lazy 策略，也就是直到内存页面被访问才实际进行页表操作。 比如，一个程序被执行时，进程的代码段理论上需要从磁盘加载到内存。但是 os 并不会马上这样做， 而是会保存 .text 段在磁盘的位置信息，在这些代码第一次被执行时才完成从磁盘的加载操作。

+ 这样做有哪些好处？

其实，我们的 mmap 也可以采取 Lazy 策略，比如：一个用户进程先后申请了 10G 的内存空间， 然后用了其中 1M 就直接退出了。按照现在的做法，我们显然亏大了，进行了很多没有意义的页表操作。

+ 处理 10G 连续的内存页面，对应的 SV39 页表大致占用多少内存 (估算数量级即可)？

+ 请简单思考如何才能实现 Lazy 策略，缺页时又如何处理？描述合理即可，不需要考虑实现。

缺页的另一个常见原因是 swap 策略，也就是内存页面可能被换到磁盘上了，导致对应页面失效。

+ 此时页面失效如何表现在页表项(PTE)上？

3. 双页表与单页表

为了防范侧信道攻击，我们的 os 使用了双页表。但是传统的设计一直是单页表的，也就是说， 用户线程和对应的内核线程共用同一张页表，只不过内核对应的地址只允许在内核态访问。 (备注：这里的单/双的说法仅为自创的通俗说法，并无这个名词概念，详情见 KPTI )

+ 在单页表情况下，如何更换页表？

+ 单页表情况下，如何控制用户态无法访问内核页面？（tips:看看上一题最后一问）

+ 单页表有何优势？（回答合理即可）

+ 双页表实现下，何时需要更换页表？假设你写一个单页表操作系统，你会选择何时更换页表（回答合理即可）？
